{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction using distant supervision: experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney and Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Building a classifier](#Building-a-classifier)\n",
    "  1. [Featurizers](#Featurizers)\n",
    "  1. [Experiments](#Experiments)\n",
    "1. [Analysis](#Analysis)\n",
    "  1. [Examining the trained models](#Examining-the-trained-models)\n",
    "  1. [Discovering new relation instances](#Discovering-new-relation-instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "OK, it's time to get (halfway) serious. Let's apply real machine learning to train a classifier on the training data, and see how it performs on the test data. We'll begin with one of the simplest machine learning setups: a bag-of-words feature representation, and a linear model trained using logistic regression.\n",
    "\n",
    "Just like we did in the unit on [supervised sentiment analysis](https://github.com/cgpotts/cs224u/blob/master/sst_02_hand_built_features.ipynb), we'll leverage the `sklearn` library, and we'll introduce functions for featurizing instances, training models, making predictions, and evaluating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import rel_ext\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system seed is relevant for this notebook.\n",
    "\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ext_data_home = os.path.join('data', 'rel_ext_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following steps, we build up the dataset we'll use for experiments; it unites a corpus and a knowledge base in the way we described in [the previous notebook](rel_ext_01_task.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'washington_post_test.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'Atsuko_filtered_KB.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(corpus):\n",
    "        related_pairs = set()\n",
    "        matches_found = []\n",
    "        for ex in corpus.examples:\n",
    "            if kb.get_triples_for_entities(ex.entity_1, ex.entity_2):\n",
    "                related_pairs.add((ex.entity_1, ex.entity_2))\n",
    "            if kb.get_triples_for_entities(ex.entity_2, ex.entity_1):\n",
    "                related_pairs.add((ex.entity_2, ex.entity_1))\n",
    "                \n",
    "        for pair in related_pairs:\n",
    "            match = kb.get_triples_for_entities(pair[0], pair[1])\n",
    "            matches_found.append(match)\n",
    "        return matches_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = find_match(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[KBTriple(rel='adjoins', sbj='Africa', obj='Europe')],\n",
       " [KBTriple(rel='contains', sbj='New_York', obj='Bard_College')],\n",
       " [KBTriple(rel='contains', sbj='Israel', obj='Jerusalem'),\n",
       "  KBTriple(rel='capital', sbj='Israel', obj='Jerusalem')],\n",
       " [KBTriple(rel='adjoins', sbj='Georgia', obj='South_Carolina')],\n",
       " [KBTriple(rel='contains', sbj='Hubei', obj='Xiantao')],\n",
       " [KBTriple(rel='contains', sbj='Bangladesh', obj='Dhaka'),\n",
       "  KBTriple(rel='capital', sbj='Bangladesh', obj='Dhaka')],\n",
       " [KBTriple(rel='adjoins', sbj='Australia', obj='Indonesia')],\n",
       " [KBTriple(rel='adjoins', sbj='Saudi_Arabia', obj='United_Arab_Emirates')],\n",
       " [KBTriple(rel='contains', sbj='Queensland', obj='Brisbane'),\n",
       "  KBTriple(rel='capital', sbj='Queensland', obj='Brisbane')],\n",
       " [KBTriple(rel='adjoins', sbj='Somalia', obj='Kenya')]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[10: 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits up our data in a way that supports experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Corpus with 54,237 examples; KB with 22,313 triples,\n",
       " 'test': Corpus with 14,543 examples; KB with 6,262 triples,\n",
       " 'all': Corpus with 68,780 examples; KB with 28,575 triples}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['train', 'test'],\n",
    "    split_fracs=[0.80, 0.20],\n",
    "    seed=1)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizers\n",
    "\n",
    "Featurizers are functions which define the feature representation for our model. The primary input to a featurizer will be the `KBTriple` for which we are generating features. But since our features will be derived from corpus examples containing the entities of the `KBTriple`, we must also pass in a reference to a `Corpus`. And in order to make it easy to combine different featurizers, we'll also pass in a feature counter to hold the results.\n",
    "\n",
    "Here's an implementation for a very simple bag-of-words featurizer. It finds all the corpus examples containing the two entities in the `KBTriple`, breaks the phrase appearing between the two entity mentions into words, and counts the words. Note that it makes no distinction between \"forward\" and \"reverse\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define another featurizer -> w2v?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(entity_1='Kenya', entity_2='Somalia', left='The al-Qaida-linked extremist group has vowed retribution on ', mention_1='Kenya', middle=' for sending troops to ', mention_2='Somalia', right=' to fight it.', left_POS='The/DET al/PROPN -/PUNCT Qaida/PROPN -/PUNCT linked/VERB extremist/ADJ group/NOUN has/AUX vowed/VERB retribution/NOUN on/ADP', mention_1_POS='Kenya/PROPN', middle_POS='/SPACE for/ADP sending/VERB troops/NOUN to/PART', mention_2_POS='Somalia/PROPN', right_POS='/SPACE to/PART fight/VERB it/PRON ./PUNCT'),\n",
       " Example(entity_1='Kenya', entity_2='Somalia', left='NAIROBI, ', mention_1='Kenya', middle=' A police officer in ', mention_2='Somalia', right=' says a car bomb blast near a security checkpoint at the presidential palace in the capital killed at least two people.', left_POS='NAIROBI/PROPN ,/PUNCT', mention_1_POS='Kenya/PROPN', middle_POS='/SPACE A/DET police/NOUN officer/NOUN in/ADP', mention_2_POS='Somalia/PROPN', right_POS='/SPACE says/VERB a/DET car/NOUN bomb/NOUN blast/NOUN near/SCONJ a/DET security/NOUN checkpoint/NOUN at/ADP the/DET presidential/ADJ palace/NOUN in/ADP the/DET capital/NOUN killed/VERB at/ADV least/ADV two/NUM people/NOUN ./PUNCT')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = corpus.get_examples_for_entities('Kenya', 'Somalia')\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The al-Qaida-linked extremist group has vowed retribution on  Kenya  for sending troops to  Somalia  to fight it.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join((ex[0][2], ex[0][3], ex[0][4], ex[0][5], ex[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAIROBI,  Kenya  A police officer in  Somalia  says a car bomb blast near a security checkpoint at the presidential palace in the capital killed at least two people.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join((ex[1][2], ex[1][3], ex[1][4], ex[1][5], ex[1][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first of 2 examples for Kenya and Somalia is:\n",
      "Example(entity_1='Kenya', entity_2='Somalia', left='The al-Qaida-linked extremist group has vowed retribution on ', mention_1='Kenya', middle=' for sending troops to ', mention_2='Somalia', right=' to fight it.', left_POS='The/DET al/PROPN -/PUNCT Qaida/PROPN -/PUNCT linked/VERB extremist/ADJ group/NOUN has/AUX vowed/VERB retribution/NOUN on/ADP', mention_1_POS='Kenya/PROPN', middle_POS='/SPACE for/ADP sending/VERB troops/NOUN to/PART', mention_2_POS='Somalia/PROPN', right_POS='/SPACE to/PART fight/VERB it/PRON ./PUNCT')\n"
     ]
    }
   ],
   "source": [
    "corpus.show_examples_for_pair('Kenya', 'Somalia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how this featurizer works on a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KBTriple(rel='adjoins', sbj='Kenya', obj='Somalia')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ij = kb.get_triples_for_entities('Kenya', 'Somalia')\n",
    "ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBTriple(rel='adjoins', sbj='Kenya', obj='Somalia')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt = ij[0]\n",
    "kbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(entity_1='Kenya', entity_2='Somalia', left='The al-Qaida-linked extremist group has vowed retribution on ', mention_1='Kenya', middle=' for sending troops to ', mention_2='Somalia', right=' to fight it.', left_POS='The/DET al/PROPN -/PUNCT Qaida/PROPN -/PUNCT linked/VERB extremist/ADJ group/NOUN has/AUX vowed/VERB retribution/NOUN on/ADP', mention_1_POS='Kenya/PROPN', middle_POS='/SPACE for/ADP sending/VERB troops/NOUN to/PART', mention_2_POS='Somalia/PROPN', right_POS='/SPACE to/PART fight/VERB it/PRON ./PUNCT'),\n",
       " Example(entity_1='Kenya', entity_2='Somalia', left='NAIROBI, ', mention_1='Kenya', middle=' A police officer in ', mention_2='Somalia', right=' says a car bomb blast near a security checkpoint at the presidential palace in the capital killed at least two people.', left_POS='NAIROBI/PROPN ,/PUNCT', mention_1_POS='Kenya/PROPN', middle_POS='/SPACE A/DET police/NOUN officer/NOUN in/ADP', mention_2_POS='Somalia/PROPN', right_POS='/SPACE says/VERB a/DET car/NOUN bomb/NOUN blast/NOUN near/SCONJ a/DET security/NOUN checkpoint/NOUN at/ADP the/DET presidential/ADJ palace/NOUN in/ADP the/DET capital/NOUN killed/VERB at/ADV least/ADV two/NUM people/NOUN ./PUNCT')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_examples_for_entities(kbt.sbj, kbt.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 4,\n",
       "         'for': 1,\n",
       "         'sending': 1,\n",
       "         'troops': 1,\n",
       "         'to': 1,\n",
       "         'A': 1,\n",
       "         'police': 1,\n",
       "         'officer': 1,\n",
       "         'in': 1})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bag_of_words_featurizer(kbt, corpus, Counter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with adding new kinds of features just by implementing additional featurizers, following `simple_bag_of_words_featurizer` as an example.\n",
    "\n",
    "Now, in order to apply machine learning algorithms such as those provided by `sklearn`, we need a way to convert datasets of `KBTriple`s into feature matrices. The following steps achieve that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbts_by_rel, labels_by_rel = dataset.build_dataset()\n",
    "\n",
    "# featurized = dataset.featurize(kbts_by_rel, featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Now we need some functions to train models, make predictions, and evaluate the results. We'll start with `train_models()`. This function takes as arguments a dictionary of data splits, a list of featurizers, the name of the split on which to train (by default, 'train'), and a model factory, which is a function which initializes an `sklearn` classifier (by default, a logistic regression classifier). It returns a dictionary holding the featurizers, the vectorizer that was used to generate the training matrix, and a dictionary holding the trained models, one per relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = rel_ext.train_models(\n",
    "    splits, split_name='train',\n",
    "    featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes `predict()`. This function takes as arguments a dictionary of data splits, the outputs of `train_models()`, and the name of the split for which to make predictions. It returns two parallel dictionaries: one holding the predictions (grouped by relation), the other holding the true labels (again, grouped by relation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = rel_ext.predict(\n",
    "    splits, train_result, split_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions['adjoins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             examples\n",
      "relation               examples    triples    /triple\n",
      "--------               --------    -------    -------\n",
      "adjoins                     641       1283       0.50\n",
      "capital                      90        406       0.22\n",
      "contains                    271      14461       0.02\n",
      "has_spouse                    4       2419       0.00\n",
      "nationality                  14       1296       0.01\n",
      "place_of_birth                0        874       0.00\n",
      "place_of_death                4        676       0.01\n",
      "worked_at                     1        898       0.00\n"
     ]
    }
   ],
   "source": [
    "splits['train'].count_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             examples\n",
      "relation               examples    triples    /triple\n",
      "--------               --------    -------    -------\n",
      "adjoins                     145        419       0.35\n",
      "capital                      19        116       0.16\n",
      "contains                     59       4220       0.01\n",
      "has_spouse                    0        575       0.00\n",
      "nationality                   1        302       0.00\n",
      "place_of_birth                1        223       0.00\n",
      "place_of_death                0        155       0.00\n",
      "worked_at                     0        252       0.00\n"
     ]
    }
   ],
   "source": [
    "splits['test'].count_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `evaluate_predictions()`. This function takes as arguments the parallel dictionaries of predictions and true labels produced by `predict()`. It prints summary statistics for each relation, including precision, recall, and F<sub>0.5</sub>-score, and it returns the macro-averaged F<sub>0.5</sub>-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.912      0.912      0.912        419       2795\n",
      "capital                   0.972      0.905      0.958        116       2492\n",
      "contains                  0.993      0.990      0.993       4220       6596\n",
      "has_spouse                0.991      1.000      0.993        575       2951\n",
      "nationality               0.987      0.997      0.989        302       2678\n",
      "place_of_birth            0.982      0.996      0.985        223       2599\n",
      "place_of_death            0.987      1.000      0.990        155       2531\n",
      "worked_at                 0.984      1.000      0.987        252       2628\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.976      0.975      0.976       6262      25270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9757934267772528"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ext.evaluate_predictions(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we introduce `rel_ext.experiment()`, which basically chains together `rel_ext.train_models()`, `rel_ext.predict()`, and `rel_ext.evaluate_predictions()`. For convenience, this function returns the output of `rel_ext.train_models()` as its result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `rel_ext.experiment()` in its default configuration will give us a baseline result for machine-learned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jr/87t7453n4pqbkl_r5lb2mb5r0000gn/T/ipykernel_99748/3566681002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m _ = rel_ext.experiment(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     featurizers=[simple_bag_of_words_featurizer])\n",
      "\u001b[0;32m~/cs224u/rel_ext.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(splits, featurizers, train_split, test_split, model_factory, train_sampling_rate, test_sampling_rate, vectorize, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         verbose=verbose)\n\u001b[0;32m--> 598\u001b[0;31m     predictions, test_y = predict(\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mtrain_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs224u/rel_ext.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(splits, train_result, split_name, sampling_rate, vectorize)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m     \u001b[0massess_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m     \u001b[0massess_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massess_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massess_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     test_X, _ = assess_dataset.featurize(\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dev'"
     ]
    }
   ],
   "source": [
    "_ = rel_ext.experiment(\n",
    "    splits,\n",
    "    featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering how vanilla our model is, these results are quite surprisingly good! We see huge gains for every relation over our `top_k_middles_classifier` from [the previous notebook](rel_ext_01_task.ipynb#A-simple-baseline-model). This strong performance is a powerful testament to the effectiveness of even the simplest forms of machine learning.\n",
    "\n",
    "But there is still much more we can do. To make further gains, we must not treat the model as a black box. We must open it up and get visibility into what it has learned, and more importantly, where it still falls down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the trained models\n",
    "\n",
    "One important way to gain understanding of our trained model is to inspect the model weights. What features are strong positive indicators for each relation, and what features are strong negative indicators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     0.778 out\n",
      "     0.778 carried\n",
      "     0.778 allies\n",
      "     ..... .....\n",
      "    -0.806 a\n",
      "    -0.885 the\n",
      "    -1.476 \n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.438 the\n",
      "    -1.113 ,\n",
      "    -3.038 \n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.380 the\n",
      "    -0.833 ,\n",
      "    -2.511 \n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     0.000 thats\n",
      "     0.000 series\n",
      "     0.000 presence\n",
      "     ..... .....\n",
      "    -0.516 the\n",
      "    -1.550 ,\n",
      "    -3.803 \n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.423 the\n",
      "    -1.034 ,\n",
      "    -2.894 \n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.446 the\n",
      "    -1.150 ,\n",
      "    -3.107 \n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.380 the\n",
      "    -0.833 ,\n",
      "    -2.511 \n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.430 the\n",
      "    -1.067 ,\n",
      "    -2.954 \n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.442 the\n",
      "    -1.129 ,\n",
      "    -3.067 \n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.452 the\n",
      "    -1.183 ,\n",
      "    -3.165 \n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.425 the\n",
      "    -1.045 ,\n",
      "    -2.915 \n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.415 the\n",
      "    -0.995 ,\n",
      "    -2.821 \n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.380 the\n",
      "    -0.833 ,\n",
      "    -2.511 \n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.412 the\n",
      "    -0.980 ,\n",
      "    -2.794 \n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.425 the\n",
      "    -1.045 ,\n",
      "    -2.915 \n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     0.000 out\n",
      "     0.000 carried\n",
      "     0.000 allies\n",
      "     ..... .....\n",
      "    -0.397 the\n",
      "    -0.909 ,\n",
      "    -2.658 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By and large, the high-weight features for each relation are pretty intuitive — they are words that are used to express the relation in question. (The counter-intuitive results merit a bit of investigation!)\n",
    "\n",
    "The low-weight features (that is, features with large negative weights) may be a bit harder to understand. In some cases, however, they can be interpreted as features which indicate some _other_ relation which is anti-correlated with the target relation. (As an example, \"directed\" is a negative indicator for the `author` relation.)\n",
    "\n",
    "__Optional exercise:__ Investigate one of the counter-intuitive high-weight features. Find the training examples which caused the feature to be included. Given the training data, does it make sense that this feature is a good predictor for the target relation?\n",
    "\n",
    "<!--\n",
    "- SPOILER: Using `penalty='l1'` results in somewhat less intuitive feature weights, and about the same performance.\n",
    "- SPOILER: Using `penalty='l1', C=0.1` results in much more intuitive feature weights, but much worse performance.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering new relation instances\n",
    "\n",
    "Another way to gain insight into our trained models is to use them to discover new relation instances that don't currently appear in the KB. In fact, this is the whole point of building a relation extraction system: to extend an existing KB (or build a new one) using knowledge extracted from natural language text at scale. Can the models we've trained do this effectively?\n",
    "\n",
    "Because the goal is to discover new relation instances which are *true* but *absent from the KB*, we can't evaluate this capability automatically. But we can generate candidate KB triples and manually evaluate them for correctness.\n",
    "\n",
    "To do this, we'll start from corpus examples containing pairs of entities which do not belong to any relation in the KB (earlier, we described these as \"negative examples\"). We'll then apply our trained models to each pair of entities, and sort the results by probability assigned by the model, in order to find the most likely new instances for each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest probability examples for relation adjoins:\n",
      "\n",
      "     1.000 KBTriple(rel='adjoins', sbj='Ladkani', obj='about_$3,000')\n",
      "     1.000 KBTriple(rel='adjoins', sbj='about_$3,000', obj='Ladkani')\n",
      "     0.999 KBTriple(rel='adjoins', sbj='2011', obj='Sudan')\n",
      "     0.999 KBTriple(rel='adjoins', sbj='Sudan', obj='2011')\n",
      "     0.998 KBTriple(rel='adjoins', sbj='China', obj='Taiwanese')\n",
      "     0.998 KBTriple(rel='adjoins', sbj='Taiwanese', obj='China')\n",
      "     0.984 KBTriple(rel='adjoins', sbj='2010.Now_Ouyang', obj='China')\n",
      "     0.984 KBTriple(rel='adjoins', sbj='China', obj='2010.Now_Ouyang')\n",
      "     0.984 KBTriple(rel='adjoins', sbj='Italy', obj='Catholic')\n",
      "     0.984 KBTriple(rel='adjoins', sbj='Catholic', obj='Italy')\n",
      "\n",
      "Highest probability examples for relation capital:\n",
      "\n",
      "     0.838 KBTriple(rel='capital', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.838 KBTriple(rel='capital', sbj='day', obj='Omar_Marrero')\n",
      "     0.838 KBTriple(rel='capital', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.838 KBTriple(rel='capital', sbj='Omar_Marrero', obj='day')\n",
      "     0.831 KBTriple(rel='capital', sbj='Kurdish', obj='Arab')\n",
      "     0.831 KBTriple(rel='capital', sbj='US', obj='Mexico')\n",
      "     0.831 KBTriple(rel='capital', sbj='Arab', obj='Kurdish')\n",
      "     0.831 KBTriple(rel='capital', sbj='Mexico', obj='US')\n",
      "     0.831 KBTriple(rel='capital', sbj='Turkey', obj='EU')\n",
      "     0.831 KBTriple(rel='capital', sbj='EU', obj='Turkey')\n",
      "\n",
      "Highest probability examples for relation contains:\n",
      "\n",
      "     0.996 KBTriple(rel='contains', sbj='the_Silk_Road', obj='China')\n",
      "     0.996 KBTriple(rel='contains', sbj='China', obj='the_Silk_Road')\n",
      "     0.990 KBTriple(rel='contains', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.990 KBTriple(rel='contains', sbj='day', obj='Omar_Marrero')\n",
      "     0.990 KBTriple(rel='contains', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.990 KBTriple(rel='contains', sbj='Omar_Marrero', obj='day')\n",
      "     0.988 KBTriple(rel='contains', sbj='Kyiv', obj='1+1')\n",
      "     0.988 KBTriple(rel='contains', sbj='1+1', obj='Kyiv')\n",
      "     0.988 KBTriple(rel='contains', sbj='Kurdish', obj='Arab')\n",
      "     0.988 KBTriple(rel='contains', sbj='US', obj='Mexico')\n",
      "\n",
      "Highest probability examples for relation has_spouse:\n",
      "\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='day', obj='Omar_Marrero')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='Omar_Marrero', obj='day')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='Kurdish', obj='Arab')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='US', obj='Mexico')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='Arab', obj='Kurdish')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='Mexico', obj='US')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='Turkey', obj='EU')\n",
      "     0.991 KBTriple(rel='has_spouse', sbj='EU', obj='Turkey')\n",
      "\n",
      "Highest probability examples for relation nationality:\n",
      "\n",
      "     0.976 KBTriple(rel='nationality', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.976 KBTriple(rel='nationality', sbj='day', obj='Omar_Marrero')\n",
      "     0.976 KBTriple(rel='nationality', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.976 KBTriple(rel='nationality', sbj='Omar_Marrero', obj='day')\n",
      "     0.976 KBTriple(rel='nationality', sbj='Kurdish', obj='Arab')\n",
      "     0.976 KBTriple(rel='nationality', sbj='US', obj='Mexico')\n",
      "     0.976 KBTriple(rel='nationality', sbj='Arab', obj='Kurdish')\n",
      "     0.976 KBTriple(rel='nationality', sbj='Mexico', obj='US')\n",
      "     0.976 KBTriple(rel='nationality', sbj='Turkey', obj='EU')\n",
      "     0.976 KBTriple(rel='nationality', sbj='EU', obj='Turkey')\n",
      "\n",
      "Highest probability examples for relation place_of_birth:\n",
      "\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='day', obj='Omar_Marrero')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='Omar_Marrero', obj='day')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='Kurdish', obj='Arab')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='US', obj='Mexico')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='Arab', obj='Kurdish')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='Mexico', obj='US')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='Turkey', obj='EU')\n",
      "     0.978 KBTriple(rel='place_of_birth', sbj='EU', obj='Turkey')\n",
      "\n",
      "Highest probability examples for relation place_of_death:\n",
      "\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='day', obj='Omar_Marrero')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='Omar_Marrero', obj='day')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='Kurdish', obj='Arab')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='US', obj='Mexico')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='Arab', obj='Kurdish')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='Mexico', obj='US')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='Turkey', obj='EU')\n",
      "     0.970 KBTriple(rel='place_of_death', sbj='EU', obj='Turkey')\n",
      "\n",
      "Highest probability examples for relation worked_at:\n",
      "\n",
      "     0.978 KBTriple(rel='worked_at', sbj='William_Happer', obj='National_Security_Council')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='day', obj='Omar_Marrero')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='National_Security_Council', obj='William_Happer')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='Omar_Marrero', obj='day')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='Kurdish', obj='Arab')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='US', obj='Mexico')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='Arab', obj='Kurdish')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='Mexico', obj='US')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='Turkey', obj='EU')\n",
      "     0.978 KBTriple(rel='worked_at', sbj='EU', obj='Turkey')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.find_new_relation_instances(dataset, featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually some good discoveries here! The predictions for the `author` relation seem especially good. Of course, there are also plenty of bad results, and a few that are downright comical. We may hope that as we improve our models and optimize performance in our automatic evaluations, the results we observe in this manual evaluation improve as well.\n",
    "\n",
    "__Optional exercise:__ Note that every time we predict that a given relation holds between entities `X` and `Y`, we also predict, with equal confidence, that it holds between `Y` and `X`. Why? How could we fix this?\n",
    "\n",
    "\\[ [top](#Relation-extraction-using-distant-supervision) \\]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
